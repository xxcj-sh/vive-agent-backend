#!/usr/bin/env python3
"""
æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹æ¨¡å—
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import asyncio
from app.services.llm_service import LLMService
from app.models.llm_schemas import ProfileAnalysisRequest, QuestionAnsweringRequest
from app.models.llm_usage_log import LLMProvider, LLMTaskType

async def test_llm_service():
    """æµ‹è¯•LLMæœåŠ¡"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹æœåŠ¡...")
    
    try:
        # åˆ›å»ºLLMæœåŠ¡å®ä¾‹
        service = LLMService()
        
        # æµ‹è¯•1: ç”¨æˆ·èµ„æ–™åˆ†æ
        print("\nğŸ“Š æµ‹è¯•ç”¨æˆ·èµ„æ–™åˆ†æ...")
        profile_request = ProfileAnalysisRequest(
            user_id="test_user_001",
            profile_data={
                "name": "å¼ ä¸‰",
                "age": 28,
                "interests": ["é˜…è¯»", "æ—…è¡Œ", "æ‘„å½±"],
                "occupation": "è½¯ä»¶å·¥ç¨‹å¸ˆ",
                "location": "åŒ—äº¬"
            }
        )
        
        result = await service.analyze_user_profile(profile_request)
        print(f"âœ… ç”¨æˆ·èµ„æ–™åˆ†æç»“æœ: {result}")
        
        # æµ‹è¯•2: å…´è¶£åˆ†æ
        print("\nğŸ¯ æµ‹è¯•å…´è¶£åˆ†æ...")
        interest_request = ProfileAnalysisRequest(
            user_id="test_user_001",
            profile_data={
                "interests": ["é˜…è¯»", "æ—…è¡Œ", "æ‘„å½±", "ç¾é£Ÿ", "éŸ³ä¹"],
                "recent_activities": ["å‘¨æœ«å»æ•…å®«æ‹ç…§", "å‚åŠ è¯»ä¹¦ä¼š", "å­¦ä¹ æ³•è¯­"]
            }
        )
        
        result = await service.analyze_user_interests(interest_request)
        print(f"âœ… å…´è¶£åˆ†æç»“æœ: {result}")
        
        # æµ‹è¯•3: é—®é¢˜å›ç­”
        print("\nâ“ æµ‹è¯•é—®é¢˜å›ç­”...")
        qa_request = QuestionAnsweringRequest(
            user_id="test_user_001",
            question="æˆ‘åº”è¯¥å¦‚ä½•æå‡æˆ‘çš„æ‘„å½±æŠ€å·§ï¼Ÿ",
            context={
                "user_interests": ["æ‘„å½±", "æ—…è¡Œ"],
                "skill_level": "ä¸­çº§"
            }
        )
        
        result = await service.answer_question(qa_request)
        print(f"âœ… é—®é¢˜å›ç­”ç»“æœ: {result}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_llm_service())