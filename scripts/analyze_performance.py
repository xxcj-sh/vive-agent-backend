"""
æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½åˆ†æå·¥å…·
ç”¨äºåˆ†æå’Œç›‘æ§æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œè¯†åˆ«æ…¢æŸ¥è¯¢å’Œä¼˜åŒ–æœºä¼š
"""

import logging
import time
from datetime import datetime, timedelta
from sqlalchemy import text, create_engine
from sqlalchemy.orm import Session
from app.config import settings
from typing import Dict, List, Any, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DatabasePerformanceAnalyzer:
    """æ•°æ®åº“æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.engine = create_engine(settings.computed_database_url)
    
    def analyze_slow_queries(self, min_execution_time: float = 0.1) -> List[Dict[str, Any]]:
        """
        åˆ†ææ…¢æŸ¥è¯¢ï¼ˆéœ€è¦MySQLæ…¢æŸ¥è¯¢æ—¥å¿—æ”¯æŒï¼‰
        
        Args:
            min_execution_time: æœ€å°æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ…¢æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            with self.engine.connect() as conn:
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
                result = conn.execute(text("SHOW VARIABLES LIKE 'slow_query_log%'"))
                slow_log_settings = dict(result.fetchall())
                
                logger.info("æ…¢æŸ¥è¯¢æ—¥å¿—è®¾ç½®:")
                for key, value in slow_log_settings.items():
                    logger.info(f"  {key}: {value}")
                
                # è·å–æœ€è¿‘çš„æ…¢æŸ¥è¯¢
                result = conn.execute(text("""
                    SELECT 
                        start_time,
                        query_time,
                        lock_time,
                        rows_sent,
                        rows_examined,
                        sql_text
                    FROM mysql.slow_log 
                    WHERE start_time >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                    AND query_time >= :min_time
                    ORDER BY query_time DESC
                    LIMIT 20
                """), {"min_time": timedelta(seconds=min_execution_time)})
                
                slow_queries = []
                for row in result.fetchall():
                    slow_queries.append({
                        'start_time': row[0],
                        'query_time': float(row[1].total_seconds()) if hasattr(row[1], 'total_seconds') else float(row[1]),
                        'lock_time': float(row[2].total_seconds()) if hasattr(row[2], 'total_seconds') else float(row[2]),
                        'rows_sent': row[3],
                        'rows_examined': row[4],
                        'sql_text': row[5][:500]  # é™åˆ¶é•¿åº¦
                    })
                
                return slow_queries
                
        except Exception as e:
            logger.error(f"æ…¢æŸ¥è¯¢åˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def analyze_table_sizes(self) -> List[Dict[str, Any]]:
        """
        åˆ†æè¡¨å¤§å°å’Œè®°å½•æ•°
        
        Returns:
            è¡¨ä¿¡æ¯åˆ—è¡¨
        """
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT 
                        table_name,
                        table_rows,
                        data_length,
                        index_length,
                        data_length + index_length as total_size,
                        ROUND(data_length / 1024 / 1024, 2) as data_mb,
                        ROUND(index_length / 1024 / 1024, 2) as index_mb,
                        ROUND((data_length + index_length) / 1024 / 1024, 2) as total_mb
                    FROM information_schema.tables 
                    WHERE table_schema = DATABASE()
                    ORDER BY total_size DESC
                """))
                
                tables = []
                for row in result.fetchall():
                    tables.append({
                        'table_name': row[0],
                        'table_rows': row[1],
                        'data_length': row[2],
                        'index_length': row[3],
                        'total_size': row[4],
                        'data_mb': row[5],
                        'index_mb': row[6],
                        'total_mb': row[7]
                    })
                
                return tables
                
        except Exception as e:
            logger.error(f"è¡¨å¤§å°åˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def analyze_index_usage(self) -> List[Dict[str, Any]]:
        """
        åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
        
        Returns:
            ç´¢å¼•ä½¿ç”¨ä¿¡æ¯åˆ—è¡¨
        """
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT 
                        table_name,
                        index_name,
                        cardinality,
                        sub_part,
                        packed,
                        nullable,
                        index_type,
                        comment
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE()
                    ORDER BY table_name, index_name
                """))
                
                indexes = {}
                for row in result.fetchall():
                    table_name = row[0]
                    index_name = row[1]
                    
                    if table_name not in indexes:
                        indexes[table_name] = []
                    
                    indexes[table_name].append({
                        'index_name': index_name,
                        'cardinality': row[2],
                        'sub_part': row[3],
                        'packed': row[4],
                        'nullable': row[5],
                        'index_type': row[6],
                        'comment': row[7]
                    })
                
                # åˆ†æç´¢å¼•æ•ˆç‡
                index_analysis = []
                for table_name, table_indexes in indexes.items():
                    for index in table_indexes:
                        # è·å–è¡¨çš„æ€»è¡Œæ•°
                        result = conn.execute(text(f"""
                            SELECT table_rows 
                            FROM information_schema.tables 
                            WHERE table_schema = DATABASE() 
                            AND table_name = '{table_name}'
                        """))
                        table_rows = result.scalar() or 0
                        
                        # è®¡ç®—ç´¢å¼•é€‰æ‹©æ€§
                        cardinality = index['cardinality'] or 0
                        selectivity = (cardinality / table_rows * 100) if table_rows > 0 else 0
                        
                        index_analysis.append({
                            'table_name': table_name,
                            'index_name': index['index_name'],
                            'cardinality': cardinality,
                            'table_rows': table_rows,
                            'selectivity': round(selectivity, 2),
                            'efficiency': 'HIGH' if selectivity > 80 else 'MEDIUM' if selectivity > 30 else 'LOW'
                        })
                
                return index_analysis
                
        except Exception as e:
            logger.error(f"ç´¢å¼•ä½¿ç”¨åˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def analyze_query_patterns(self) -> Dict[str, Any]:
        """
        åˆ†æå¸¸è§æŸ¥è¯¢æ¨¡å¼
        
        Returns:
            æŸ¥è¯¢æ¨¡å¼åˆ†æç»“æœ
        """
        try:
            with self.engine.connect() as conn:
                # åˆ†æè¡¨è®¿é—®é¢‘ç‡ï¼ˆå¦‚æœå¯ç”¨äº†æ€§èƒ½æ¨¡å¼ï¼‰
                result = conn.execute(text("""
                    SELECT 
                        object_schema,
                        object_name,
                        count_read,
                        count_write,
                        count_fetch,
                        count_insert,
                        count_update,
                        count_delete
                    FROM performance_schema.table_io_waits_summary_by_table 
                    WHERE object_schema = DATABASE()
                    ORDER BY count_read + count_write DESC
                    LIMIT 20
                """))
                
                table_io_stats = []
                for row in result.fetchall():
                    table_io_stats.append({
                        'schema': row[0],
                        'table': row[1],
                        'reads': row[2],
                        'writes': row[3],
                        'fetches': row[4],
                        'inserts': row[5],
                        'updates': row[6],
                        'deletes': row[7]
                    })
                
                return {
                    'table_io_stats': table_io_stats,
                    'analysis_time': datetime.now()
                }
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ¨¡å¼åˆ†æå¤±è´¥: {str(e)}")
            return {'table_io_stats': [], 'analysis_time': datetime.now()}
    
    def benchmark_common_queries(self) -> List[Dict[str, Any]]:
        """
        åŸºå‡†æµ‹è¯•å¸¸è§æŸ¥è¯¢
        
        Returns:
            æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ
        """
        # å¸¸è§æŸ¥è¯¢æ¨¡å¼
        test_queries = [
            {
                'name': 'ç”¨æˆ·æ‰‹æœºå·æŸ¥è¯¢',
                'sql': "SELECT * FROM users WHERE phone = :param LIMIT 1",
                'params': {'param': '13800138000'}
            },
            {
                'name': 'ç”¨æˆ·å¡ç‰‡ç»„åˆæŸ¥è¯¢',
                'sql': """
                    SELECT * FROM user_cards 
                    WHERE user_id = :user_id 
                    AND scene_type = :scene_type 
                    AND role_type = :role_type 
                    AND is_deleted = 0 
                    AND is_active = 1
                """,
                'params': {
                    'user_id': 'test_user_123',
                    'scene_type': 'housing',
                    'role_type': 'housing_seeker'
                }
            },
            {
                'name': 'è¯é¢˜åˆ—è¡¨æŸ¥è¯¢',
                'sql': """
                    SELECT tc.*, u.nick_name as creator_nickname 
                    FROM topic_cards tc
                    JOIN users u ON tc.user_id = u.id
                    WHERE tc.is_deleted = 0 AND tc.is_active = 1
                    ORDER BY tc.created_at DESC
                    LIMIT 10
                """,
                'params': {}
            },
            {
                'name': 'ç”¨æˆ·è¿æ¥æŸ¥è¯¢',
                'sql': """
                    SELECT * FROM user_connections 
                    WHERE from_user_id = :user_id 
                    AND status = 'ACCEPTED'
                    ORDER BY created_at DESC
                """,
                'params': {'user_id': 'test_user_123'}
            },
            {
                'name': 'èŠå¤©æ¶ˆæ¯æŸ¥è¯¢',
                'sql': """
                    SELECT * FROM chat_messages 
                    WHERE user_id = :user_id 
                    AND created_at >= :start_time
                    ORDER BY created_at DESC
                    LIMIT 50
                """,
                'params': {
                    'user_id': 'test_user_123',
                    'start_time': datetime.now() - timedelta(hours=24)
                }
            }
        ]
        
        benchmark_results = []
        
        try:
            with self.engine.connect() as conn:
                for query_info in test_queries:
                    try:
                        # é¢„çƒ­æŸ¥è¯¢è®¡åˆ’ç¼“å­˜
                        conn.execute(text(query_info['sql']), query_info['params'])
                        
                        # æ‰§è¡Œå¤šæ¬¡å–å¹³å‡å€¼
                        execution_times = []
                        for _ in range(5):
                            start_time = time.time()
                            result = conn.execute(text(query_info['sql']), query_info['params'])
                            list(result.fetchall())  # å¼ºåˆ¶è·å–æ‰€æœ‰ç»“æœ
                            end_time = time.time()
                            execution_times.append(end_time - start_time)
                        
                        avg_time = sum(execution_times) / len(execution_times)
                        min_time = min(execution_times)
                        max_time = max(execution_times)
                        
                        benchmark_results.append({
                            'query_name': query_info['name'],
                            'avg_time': round(avg_time, 4),
                            'min_time': round(min_time, 4),
                            'max_time': round(max_time, 4),
                            'execution_times': [round(t, 4) for t in execution_times]
                        })
                        
                    except Exception as e:
                        logger.error(f"æŸ¥è¯¢åŸºå‡†æµ‹è¯•å¤±è´¥ - {query_info['name']}: {str(e)}")
                        benchmark_results.append({
                            'query_name': query_info['name'],
                            'error': str(e)
                        })
                
                return benchmark_results
                
        except Exception as e:
            logger.error(f"åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
            return []
    
    def generate_performance_report(self) -> str:
        """
        ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š
        
        Returns:
            æ€§èƒ½åˆ†ææŠ¥å‘Š
        """
        logger.info("å¼€å§‹ç”Ÿæˆæ•°æ®åº“æ€§èƒ½åˆ†ææŠ¥å‘Š...")
        
        # æ”¶é›†å„é¡¹åˆ†ææ•°æ®
        slow_queries = self.analyze_slow_queries()
        table_sizes = self.analyze_table_sizes()
        index_usage = self.analyze_index_usage()
        query_patterns = self.analyze_query_patterns()
        benchmarks = self.benchmark_common_queries()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("=" * 60)
        report.append("æ•°æ®åº“æ€§èƒ½åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # è¡¨å¤§å°åˆ†æ
        report.append("ğŸ“Š è¡¨å¤§å°åˆ†æ")
        report.append("-" * 30)
        total_size_mb = sum(table['total_mb'] for table in table_sizes if table['total_mb'] is not None)
        report.append(f"æ€»æ•°æ®åº“å¤§å°: {total_size_mb:.2f} MB")
        report.append("")
        
        for table in table_sizes[:10]:  # æ˜¾ç¤ºå‰10ä¸ªå¤§è¡¨
            if table['table_rows'] is not None and table['data_mb'] is not None and table['index_mb'] is not None:
                report.append(f"è¡¨: {table['table_name']}")
                report.append(f"  è¡Œæ•°: {table['table_rows']:,}")
                report.append(f"  æ•°æ®å¤§å°: {table['data_mb']:.2f} MB")
                report.append(f"  ç´¢å¼•å¤§å°: {table['index_mb']:.2f} MB")
                report.append(f"  æ€»å¤§å°: {table['total_mb']:.2f} MB")
                report.append("")
        
        # ç´¢å¼•ä½¿ç”¨åˆ†æ
        report.append("ğŸ” ç´¢å¼•ä½¿ç”¨åˆ†æ")
        report.append("-" * 30)
        
        high_efficiency = [idx for idx in index_usage if idx['efficiency'] == 'HIGH']
        low_efficiency = [idx for idx in index_usage if idx['efficiency'] == 'LOW']
        
        report.append(f"é«˜æ•ˆç´¢å¼•æ•°é‡: {len(high_efficiency)}")
        report.append(f"ä½æ•ˆç´¢å¼•æ•°é‡: {len(low_efficiency)}")
        report.append("")
        
        if low_efficiency:
            report.append("ä½æ•ˆç´¢å¼•åˆ—è¡¨:")
            for idx in low_efficiency[:5]:  # æ˜¾ç¤ºå‰5ä¸ªä½æ•ˆç´¢å¼•
                report.append(f"  è¡¨: {idx['table_name']}, ç´¢å¼•: {idx['index_name']}")
                report.append(f"  é€‰æ‹©æ€§: {idx['selectivity']:.2f}%")
                report.append("")
        
        # æŸ¥è¯¢åŸºå‡†æµ‹è¯•
        report.append("âš¡ æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•")
        report.append("-" * 30)
        
        for benchmark in benchmarks:
            if 'error' not in benchmark:
                report.append(f"æŸ¥è¯¢: {benchmark['query_name']}")
                report.append(f"  å¹³å‡æ—¶é—´: {benchmark['avg_time']:.4f} ç§’")
                report.append(f"  æœ€çŸ­æ—¶é—´: {benchmark['min_time']:.4f} ç§’")
                report.append(f"  æœ€é•¿æ—¶é—´: {benchmark['max_time']:.4f} ç§’")
                report.append("")
            else:
                report.append(f"æŸ¥è¯¢: {benchmark['query_name']} - é”™è¯¯: {benchmark['error']}")
                report.append("")
        
        # æ…¢æŸ¥è¯¢åˆ†æ
        if slow_queries:
            report.append("ğŸŒ æ…¢æŸ¥è¯¢åˆ†æï¼ˆæœ€è¿‘24å°æ—¶ï¼‰")
            report.append("-" * 30)
            report.append(f"å‘ç° {len(slow_queries)} ä¸ªæ…¢æŸ¥è¯¢")
            report.append("")
            
            for query in slow_queries[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæ…¢æŸ¥è¯¢
                report.append(f"æ‰§è¡Œæ—¶é—´: {query['query_time']:.4f} ç§’")
                report.append(f"é”å®šæ—¶é—´: {query['lock_time']:.4f} ç§’")
                report.append(f"æ‰«æè¡Œæ•°: {query['rows_examined']:,}")
                report.append(f"è¿”å›è¡Œæ•°: {query['rows_sent']:,}")
                report.append(f"SQL: {query['sql_text'][:200]}...")
                report.append("")
        
        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        report.append("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®")
        report.append("-" * 30)
        
        recommendations = []
        
        # åŸºäºä½æ•ˆç´¢å¼•çš„å»ºè®®
        if low_efficiency:
            recommendations.append("â€¢ è€ƒè™‘é‡å»ºæˆ–åˆ é™¤ä½æ•ˆç´¢å¼•ï¼Œç‰¹åˆ«æ˜¯é€‰æ‹©æ€§ä½äº30%çš„ç´¢å¼•")
        
        # åŸºäºè¡¨å¤§å°çš„å»ºè®®
        large_tables = [t for t in table_sizes if t['total_mb'] > 100]
        if large_tables:
            recommendations.append("â€¢ å¯¹å¤§è¡¨è€ƒè™‘åˆ†åŒºæˆ–å½’æ¡£å†å²æ•°æ®")
        
        # åŸºäºæ…¢æŸ¥è¯¢çš„å»ºè®®
        if slow_queries:
            recommendations.append("â€¢ ä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼Œè€ƒè™‘æ·»åŠ åˆé€‚çš„å¤åˆç´¢å¼•")
            recommendations.append("â€¢ æ£€æŸ¥æŸ¥è¯¢è¯­å¥æ˜¯å¦å­˜åœ¨å…¨è¡¨æ‰«æ")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "â€¢ å®šæœŸæ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆANALYZE TABLEï¼‰",
            "â€¢ ç›‘æ§ç´¢å¼•ç¢ç‰‡ç‡ï¼Œå¿…è¦æ—¶é‡å»ºç´¢å¼•",
            "â€¢ è€ƒè™‘ä¸ºé¢‘ç¹æŸ¥è¯¢çš„åˆ—ç»„åˆåˆ›å»ºå¤åˆç´¢å¼•",
            "â€¢ å®šæœŸæ¸…ç†ä¸å†éœ€è¦çš„å†å²æ•°æ®"
        ])
        
        for rec in recommendations:
            report.append(rec)
        
        report.append("")
        report.append("=" * 60)
        report.append("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        report.append("=" * 60)
        
        return "\n".join(report)
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.engine:
            self.engine.dispose()

def main():
    """ä¸»å‡½æ•°"""
    analyzer = DatabasePerformanceAnalyzer()
    
    try:
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        report = analyzer.generate_performance_report()
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f"database_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print("\n" + report)
        
    except Exception as e:
        logger.error(f"æ€§èƒ½åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")
    
    finally:
        analyzer.close()

if __name__ == "__main__":
    main()