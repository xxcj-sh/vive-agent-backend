"""
ç®€åŒ–ç‰ˆæ•°æ®åº“æ€§èƒ½åˆ†æå·¥å…·
ç”¨äºéªŒè¯ç´¢å¼•ä¼˜åŒ–æ•ˆæœ
"""

import logging
from datetime import datetime
from sqlalchemy import text, create_engine
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('/Users/liukun/Documents/workspace/codebase/VMatch/vive-agent-backend')

from app.config import settings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def analyze_table_indexes():
    """åˆ†æç°æœ‰è¡¨çš„ç´¢å¼•æƒ…å†µ"""
    
    engine = create_engine(settings.computed_database_url)
    
    # éœ€è¦åˆ†æçš„è¡¨
    tables = [
        'users', 'user_cards', 'topic_cards', 'vote_cards', 
        'matches', 'user_connections', 'chat_messages',
        'topic_discussions', 'vote_records', 'topic_opinion_summaries',
        'user_card_topic_relations', 'user_card_vote_relations'
    ]
    
    with engine.connect() as conn:
        try:
            logger.info("å¼€å§‹åˆ†æç°æœ‰ç´¢å¼•æƒ…å†µ...")
            
            for table in tables:
                try:
                    # è·å–è¡¨çš„ç´¢å¼•ä¿¡æ¯
                    result = conn.execute(text(f"""
                        SELECT 
                            INDEX_NAME,
                            COLUMN_NAME,
                            NON_UNIQUE,
                            SEQ_IN_INDEX,
                            COLLATION,
                            CARDINALITY,
                            SUB_PART
                        FROM INFORMATION_SCHEMA.STATISTICS 
                        WHERE TABLE_SCHEMA = DATABASE() 
                        AND TABLE_NAME = '{table}'
                        ORDER BY INDEX_NAME, SEQ_IN_INDEX
                    """))
                    
                    indexes = result.fetchall()
                    
                    if indexes:
                        logger.info(f"\nğŸ“Š è¡¨ {table} çš„ç´¢å¼•:")
                        current_index = ""
                        for idx in indexes:
                            if idx[0] != current_index:
                                current_index = idx[0]
                                logger.info(f"  ğŸ“Œ ç´¢å¼•: {idx[0]} (å”¯ä¸€: {'å¦' if idx[2] else 'æ˜¯'})")
                            logger.info(f"     åˆ—: {idx[1]} (ä½ç½®: {idx[3]})")
                    else:
                        logger.info(f"âš ï¸ è¡¨ {table} æš‚æ— ç´¢å¼•")
                        
                except Exception as e:
                    logger.error(f"åˆ†æè¡¨ {table} å¤±è´¥: {str(e)}")
            
            logger.info("\nâœ… ç´¢å¼•åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"ç´¢å¼•åˆ†æå¤±è´¥: {str(e)}")
            raise e
        
        finally:
            conn.close()
    
    engine.dispose()

def analyze_table_sizes():
    """åˆ†æè¡¨å¤§å°å’Œè®°å½•æ•°"""
    
    engine = create_engine(settings.computed_database_url)
    
    with engine.connect() as conn:
        try:
            logger.info("å¼€å§‹åˆ†æè¡¨å¤§å°...")
            
            result = conn.execute(text("""
                SELECT 
                    table_name,
                    table_rows,
                    data_length,
                    index_length,
                    data_length + index_length as total_size,
                    ROUND(data_length / 1024 / 1024, 2) as data_mb,
                    ROUND(index_length / 1024 / 1024, 2) as index_mb,
                    ROUND((data_length + index_length) / 1024 / 1024, 2) as total_mb
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
                ORDER BY total_size DESC
            """))
            
            tables = result.fetchall()
            
            logger.info("\nğŸ“Š è¡¨å¤§å°åˆ†æ:")
            logger.info("-" * 50)
            
            total_size_mb = 0
            for table in tables:
                table_name = table[0]
                table_rows = table[1] or 0
                data_mb = table[5] or 0
                index_mb = table[6] or 0
                total_mb = table[7] or 0
                total_size_mb += total_mb
                
                logger.info(f"è¡¨: {table_name}")
                logger.info(f"  è¡Œæ•°: {table_rows:,}")
                logger.info(f"  æ•°æ®å¤§å°: {data_mb:.2f} MB")
                logger.info(f"  ç´¢å¼•å¤§å°: {index_mb:.2f} MB")
                logger.info(f"  æ€»å¤§å°: {total_mb:.2f} MB")
                logger.info("")
            
            logger.info(f"æ€»æ•°æ®åº“å¤§å°: {total_size_mb:.2f} MB")
            
        except Exception as e:
            logger.error(f"è¡¨å¤§å°åˆ†æå¤±è´¥: {str(e)}")
        
        finally:
            conn.close()
    
    engine.dispose()

def benchmark_common_queries():
    """åŸºå‡†æµ‹è¯•å¸¸è§æŸ¥è¯¢"""
    
    engine = create_engine(settings.computed_database_url)
    
    # å¸¸è§æŸ¥è¯¢æ¨¡å¼ï¼ˆåªæµ‹è¯•å­˜åœ¨çš„è¡¨ï¼‰
    test_queries = [
        {
            'name': 'ç”¨æˆ·æ‰‹æœºå·æŸ¥è¯¢',
            'sql': "SELECT * FROM users WHERE phone = %s LIMIT 1",
            'params': ('13800138000',)
        },
        {
            'name': 'ç”¨æˆ·å¾®ä¿¡openidæŸ¥è¯¢',
            'sql': "SELECT * FROM users WHERE wechat_open_id = %s LIMIT 1",
            'params': ('test_openid_123',)
        },
        {
            'name': 'ç”¨æˆ·å¡ç‰‡ç»„åˆæŸ¥è¯¢',
            'sql': """
                SELECT * FROM user_cards 
                WHERE user_id = %s 
                AND scene_type = %s 
                AND role_type = %s 
                AND is_deleted = 0 
                AND is_active = 1
            """,
            'params': ('test_user_123', 'housing', 'housing_seeker')
        },
        {
            'name': 'è¯é¢˜åˆ—è¡¨æŸ¥è¯¢',
            'sql': """
                SELECT tc.*, u.nick_name as creator_nickname 
                FROM topic_cards tc
                JOIN users u ON tc.user_id = u.id
                WHERE tc.is_deleted = 0 AND tc.is_active = 1
                ORDER BY tc.created_at DESC
                LIMIT 10
            """,
            'params': ()
        },
        {
            'name': 'ç”¨æˆ·è¿æ¥æŸ¥è¯¢',
            'sql': """
                SELECT * FROM user_connections 
                WHERE from_user_id = %s 
                AND status = 'ACCEPTED'
                ORDER BY created_at DESC
            """,
            'params': ('test_user_123',)
        }
    ]
    
    benchmark_results = []
    
    with engine.connect() as conn:
        try:
            logger.info("å¼€å§‹æŸ¥è¯¢åŸºå‡†æµ‹è¯•...")
            
            for query_info in test_queries:
                try:
                    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                    table_check = query_info['sql'].split('FROM')[1].split(' ')[1].strip()
                    result = conn.execute(text(f"SHOW TABLES LIKE '{table_check}'"))
                    if not result.fetchone():
                        logger.info(f"âš ï¸ è¡¨ {table_check} ä¸å­˜åœ¨ï¼Œè·³è¿‡æŸ¥è¯¢: {query_info['name']}")
                        continue
                    
                    # é¢„çƒ­æŸ¥è¯¢è®¡åˆ’ç¼“å­˜
                    conn.execute(text(query_info['sql']), query_info['params'])
                    
                    # æ‰§è¡Œå¤šæ¬¡å–å¹³å‡å€¼
                    execution_times = []
                    import time
                    for _ in range(3):
                        start_time = time.time()
                        result = conn.execute(text(query_info['sql']), query_info['params'])
                        list(result.fetchall())  # å¼ºåˆ¶è·å–æ‰€æœ‰ç»“æœ
                        end_time = time.time()
                        execution_times.append(end_time - start_time)
                    
                    avg_time = sum(execution_times) / len(execution_times)
                    min_time = min(execution_times)
                    max_time = max(execution_times)
                    
                    benchmark_results.append({
                        'query_name': query_info['name'],
                        'avg_time': round(avg_time, 4),
                        'min_time': round(min_time, 4),
                        'max_time': round(max_time, 4),
                        'execution_times': [round(t, 4) for t in execution_times]
                    })
                    
                    logger.info(f"âœ“ {query_info['name']}: å¹³å‡ {avg_time:.4f}s, æœ€å° {min_time:.4f}s, æœ€å¤§ {max_time:.4f}s")
                    
                except Exception as e:
                    logger.error(f"æŸ¥è¯¢åŸºå‡†æµ‹è¯•å¤±è´¥ - {query_info['name']}: {str(e)}")
                    benchmark_results.append({
                        'query_name': query_info['name'],
                        'error': str(e)
                    })
            
            logger.info("\nâœ… æŸ¥è¯¢åŸºå‡†æµ‹è¯•å®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        
        finally:
            conn.close()
    
    engine.dispose()
    
    return benchmark_results

def analyze_index_efficiency():
    """åˆ†æç´¢å¼•æ•ˆç‡"""
    
    engine = create_engine(settings.computed_database_url)
    
    with engine.connect() as conn:
        try:
            logger.info("å¼€å§‹åˆ†æç´¢å¼•æ•ˆç‡...")
            
            # è·å–æ‰€æœ‰è¡¨çš„ç´¢å¼•ä¿¡æ¯
            result = conn.execute(text("""
                SELECT 
                    TABLE_NAME,
                    INDEX_NAME,
                    COLUMN_NAME,
                    NON_UNIQUE,
                    CARDINALITY,
                    SUB_PART
                FROM INFORMATION_SCHEMA.STATISTICS 
                WHERE TABLE_SCHEMA = DATABASE()
                ORDER BY TABLE_NAME, INDEX_NAME, SEQ_IN_INDEX
            """))
            
            indexes = result.fetchall()
            
            # æŒ‰è¡¨åˆ†ç»„ç´¢å¼•
            table_indexes = {}
            for idx in indexes:
                table_name = idx[0]
                index_name = idx[1]
                
                if table_name not in table_indexes:
                    table_indexes[table_name] = {}
                
                if index_name not in table_indexes[table_name]:
                    table_indexes[table_name][index_name] = {
                        'columns': [],
                        'cardinality': idx[4],
                        'unique': not idx[3]
                    }
                
                table_indexes[table_name][index_name]['columns'].append(idx[2])
            
            # è·å–è¡¨è¡Œæ•°
            result = conn.execute(text("""
                SELECT 
                    table_name,
                    table_rows
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
            """))
            
            table_rows = dict(result.fetchall())
            
            logger.info("\nğŸ” ç´¢å¼•æ•ˆç‡åˆ†æ:")
            logger.info("-" * 50)
            
            for table_name, indexes in table_indexes.items():
                rows = table_rows.get(table_name, 0)
                logger.info(f"\nè¡¨: {table_name} (è¡Œæ•°: {rows:,})")
                
                for index_name, index_info in indexes.items():
                    cardinality = index_info['cardinality'] or 0
                    selectivity = (cardinality / rows * 100) if rows > 0 else 0
                    
                    logger.info(f"  ç´¢å¼•: {index_name}")
                    logger.info(f"    åˆ—: {', '.join(index_info['columns'])}")
                    logger.info(f"    åŸºæ•°: {cardinality:,}")
                    logger.info(f"    é€‰æ‹©æ€§: {selectivity:.2f}%")
                    logger.info(f"    æ•ˆç‡: {'é«˜' if selectivity > 80 else 'ä¸­' if selectivity > 30 else 'ä½'}")
            
        except Exception as e:
            logger.error(f"ç´¢å¼•æ•ˆç‡åˆ†æå¤±è´¥: {str(e)}")
        
        finally:
            conn.close()
    
    engine.dispose()

def main():
    """ä¸»å‡½æ•°"""
    
    logger.info("=" * 60)
    logger.info("æ•°æ®åº“æ€§èƒ½åˆ†æå·¥å…·")
    logger.info("=" * 60)
    logger.info(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    try:
        # 1. åˆ†æè¡¨å¤§å°
        analyze_table_sizes()
        
        # 2. åˆ†æç´¢å¼•æƒ…å†µ
        analyze_table_indexes()
        
        # 3. åˆ†æç´¢å¼•æ•ˆç‡
        analyze_index_efficiency()
        
        # 4. åŸºå‡†æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
        benchmark_common_queries()
        
        logger.info("\n" + "=" * 60)
        logger.info("æ€§èƒ½åˆ†æå®Œæˆï¼")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"æ€§èƒ½åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()